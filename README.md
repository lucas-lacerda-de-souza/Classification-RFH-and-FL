**Deep Learning-Based Histopathologic Classification of Head and Neck Reactive Follicular Hyperplasia and Follicular Lymphoma** 

Author: Lucas Lacerda de Souza

Year: 2025
________________________________________
**1. Project Overview**

This project implements a multimodal artificial intelligence pipeline for classifying Reactive Follicular Hyperplasia (RFH) and Follicular Lymphoma (FL).
It integrates histopathological images, clinicopathologic data, and morphometric nuclear features using XGBoost (SHAP), convolutional neural networks (CNNs) + multilayer perceptron, and explainable AI methods (Grad-CAM).
________________________________________
**2. Pipeline**

<img width="1228" height="957" alt="Captura de tela 2025-10-30 125109" src="https://github.com/user-attachments/assets/43cf2137-203f-4422-bb54-1917b7fb7962" />

________________________________________
**3. Environment and Hardware**

All experiments were performed using the following configuration:

**Operating System:** Ubuntu 20.04.1 LTS

**Python Version:** 3.12.11

**PyTorch Version:** 2.8.0 (CUDA 12.8)

**CPU:** Intel Xeon W-2295 (18 cores / 36 threads)

**RAM:** 125 GB

**GPUs:** 3 Ã— NVIDIA GeForce RTX 3090 (24 GB each)

________________________________________
**4. Environment Files**

**Channels:**

  â€¢ pytorch
  
  â€¢ nvidia
  
  â€¢ defaults
  
**Dependencies:**

  â€¢ python=3.12.11
  
  â€¢ pytorch=2.8.0
  
  â€¢ torchvision=0.19.0
  
  â€¢ torchaudio=2.8.0
  
  â€¢ cudatoolkit=12.8
  
  â€¢ numpy=1.26.4
  
  â€¢ pandas=2.2.3
  
  â€¢ scikit-learn=1.5.2
  
  â€¢ matplotlib=3.9.2
  
  â€¢ seaborn=0.13.2
  
  â€¢ pillow=10.4.0
  
  â€¢ tqdm=4.66.5
  
  â€¢ openpyxl=3.1.5
________________________________________
**5. Model Architectures**

â€¢	XGBoost +SHAP

â€¢	U-Net++

â€¢	AlexNet + Multilayer perceptron

â€¢	VGG16 + Multilayer perceptron

â€¢	ResNet18 + Multilayer perceptron

â€¢	GradCam
________________________________________
**6. Features Used**

â€¢ Patches (H&E)

â€¢ Patches (Unet++)
   
â€¢	Morphometric features (nucleus-based)

â€¢	Clinicopathologic features (age, sex, location)
________________________________________
**7. Evaluation Metrics**
   
â€¢	XGBoost + SHAP â€“ Classification (accuracy, area under the curve (AUC), F1-score, precision, recall and SHAP).

â€¢	U-Net++ (Loss, Accuracy, Precision, Recall, IoU and Dice coefficient).

â€¢	AlexNet (Loss, Accuracy, Precision, Recall, Confusion matrix (TP, FN, FP, TN), F1-score, Specificity, Receiver operating characteristic â€“ area under the curve (ROC AUC) and Cohen's Kappa).

â€¢	VGG16 (Loss, Accuracy, Precision, Recall, Confusion matrix (TP, FN, FP, TN), F1-score, Specificity, Receiver operating characteristic â€“ area under the curve (ROC AUC) and Cohen's Kappa).

â€¢	ResNet18 (Loss, Accuracy, Precision, Recall, Confusion matrix (TP, FN, FP, TN), F1-score, Specificity, Receiver operating characteristic â€“ area under the curve (ROC AUC) and Cohen's Kappa).

â€¢	GradCam - XGBoost - Classification (accuracy, area under the curve (AUC), F1-score, precision, recall). 

________________________________________
**8. Repository Structure**
   
## ðŸ“‚ Repository Structure

INFERENCE.py â€” Inference Script Example

LICENSE.txt â€” Project license

MODEL_CARD.txt â€” Description of the essential information of the study 

README.md â€” Documentation and usage instructions

REQUIREMENTS.txt â€” Dependencies


data/

patches/

â€ƒâ”œâ”€â”€ gradcam/
 
â€ƒâ”‚â€ƒâ”œâ”€â”€ heatmaps/
 
â€ƒâ”‚â€ƒâ”‚â€ƒâ””â”€â”€ heatmap.png files
 
â€ƒâ”‚â€ƒâ””â”€â”€ patches/
 
â€ƒâ”‚â€ƒâ€ƒâ””â”€â”€ patch.png files

â€ƒâ”‚â€ƒâ””â”€â”€ wsi_heatmaps/
 
â€ƒâ”‚â€ƒâ€ƒâ””â”€â”€ wsi.png files
 
â€ƒâ”œâ”€â”€ masks/
 
â€ƒâ”‚â€ƒâ”œâ”€â”€ train/
 
â€ƒâ”‚â€ƒâ”œâ”€â”€ val/
 
â€ƒâ”‚â€ƒâ””â”€â”€ test/
 
â€ƒâ”‚â€ƒâ€ƒâ””â”€â”€ mask.png files
 
â€ƒâ””â”€â”€ patches/
 
â€ƒâ€ƒâ”œâ”€â”€ train/
  
â€ƒâ€ƒâ”œâ”€â”€ val/
  
â€ƒâ€ƒâ””â”€â”€ test/
  
â€ƒâ€ƒâ€ƒâ””â”€â”€ patch.png files
   
 models/

â€ƒâ”œâ”€â”€ multimodal_alexnet_patch_level.py
 
â€ƒâ”œâ”€â”€ multimodal_alexnet_patient_level.py
 
â€ƒâ”œâ”€â”€ multimodal_resnet18_patch_level.py
 
â€ƒâ”œâ”€â”€ multimodal_resnet18_patient_level.py
 
â€ƒâ”œâ”€â”€ multimodal_vgg16_patch_level.py
 
â€ƒâ”œâ”€â”€ multimodal_vgg16_patient_level.py
 
â€ƒâ”œâ”€â”€ segmentation_unet++.py
 
â€ƒâ”œâ”€â”€ xgboost_classification_cpc_mpa.R
 
â€ƒâ””â”€â”€ xgboost_classification_gradcam.R

results/

â€ƒâ””â”€â”€ metrics

________________________________________

**9. Run models and reproduce tables**



<img width="1600" height="461" alt="image" src="https://github.com/user-attachments/assets/12d4a0a9-1d72-4108-9b5c-78f71f31c730" />



________________________________________

**10. Installation**

git clone https://github.com/lucas-lacerda-de-souza/Classification-RFH-and-FL.git
cd Classification-RFH-and-FL

________________________________________

**11. Quick Start Guide**

**11.1. Clone the repository**

git clone https://github.com/lucas-lacerda-de-souza/Classification-RFH-and-FL.git
cd Classification-RFH-and-FL

**11.2. Create and activate the environment**

conda env create -f environment.yml
conda activate rfh-fl-ai

**11.3. Run inference**

python inference.py --input_dir ./data/test/ --output_dir ./results/

**11.4. Generate Grad-CAM heatmaps**

python scripts/visualize_gradcam.py \
  --model resnet18 \
  --input_dir ./data/test/ \
  --output_dir ./gradcam/heatmaps/
________________________________________

**12. Compliance with TRIPOD-AI and CLAIM 2024 Guidelines**

This repository has been structured to meet the TRIPOD-AI (Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis â€“ 
AI extension) and CLAIM 2024 (Checklist for Artificial Intelligence in Medical Imaging) requirements for transparent and reproducible AI in healthcare.

**Data Source and Splits**

Detailed in README.md â†’ Dataset Organization and METHODS.md.
Data divided into 80% training, 10% validation, and 10% testing.
Two independent external validation cohorts used to assess generalizability.

**Model Architecture and Training**

Documented in /models and individual training scripts.
Includes optimizer (AdamW), learning rate, batch size, epochs, and loss functions.

**Performance Metrics**

Internal and external validation results summarized in /results
Cross-institutional evaluation demonstrates robustness to domain shifts.

**Interpretability and Explainability**

SHAP feature importance for XGBoost models and Grad-CAM heatmaps for CNNs included.
Code and examples available in /models and /data.

**Clinical and Biological Relevance**

Described in MODEL_CARD.md â†’ Intended Use.
Designed to assist diagnostic workflows, not to replace expert evaluation.

**Limitations and Potential Biases**

Outlined in MODEL_CARD.
Includes dataset size, center-specific staining differences, and potential bias from single-institution data predominance.

**Ethical Considerations**

Discussed in MODEL_CARD.md â†’ Ethical and Practical Considerations.
Model not intended for autonomous clinical use; human oversight required at all stages.

________________________________________

**13. Ethics**

This study was approved by the Ethics Committee of the Piracicaba Dental School, University of Campinas, Piracicaba, Brazil (protocol no. 67064422.9.1001.5418), 
and by the West of Scotland Research Ethics Service (20/WS/0017). The study was performed according to the clinical standards of the 1975 and 1983 Declaration of Helsinki. 
Written consent was not required as data was collected from surplus archived tissue. Data collected were fully anonymised.

________________________________________

**14. Data availability**

All the data derived from this study are included in the manuscript. We are unable to share the whole slide images and clinical data, due to restrictions in the 
ethics applications. However, we created synthetic slides to show the structure of the project.

________________________________________

**15. Code availability**

We have made the codes publicly available online, along with model weights (https://github.com/lucas-lacerda-de-souza/Classification-RFH-and-FL). All code was written 
with Python Python 3.12.11, along with PyTorch 2.8.0. The full implementation of the model, including the code and documentation, has been deposited in the Zenodo repository 
and is publicly available ([https://doi.org/10.1234/RFH_vs_FL_AI_pipeline](https://doi.org/10.5281/zenodo.17474399)). 

________________________________________
**16. Citation**

@article{delasouza2025,
  title={Deep Learning-Based Histopathologic Classification of Head and Neck Reactive Follicular Hyperplasia and Follicular Lymphoma},
  author={Souza, Lucas Lacerda de, Chen, Zhiyang [â€¦] Khurram, Syed Ali and Vargas, Pablo Agustin},
  journal={npj digital medicine},
  year={2025},
  publisher={Nature Publishing Group UK London}
}
________________________________________
**17. License**

MIT License Â© 2025 Lucas Lacerda de Souza

